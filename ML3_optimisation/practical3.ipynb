{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Practical Session 3 - Optimization\n",
    "\n",
    "In this session we will talk about optimization in general and its application to machine learning.\n",
    "\n",
    "First we will look into a general setting. Let us simply minimize the function :\n",
    " $ f(x) = x^2 $ when starting from $x_0=2$\n",
    " \n",
    " A one-liner for that is to use scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "\n",
    "x_0 = 2\n",
    "\n",
    "result = scipy.optimize.minimize(f, x_0)\n",
    "print(result.x)  # this should print a value very close to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Implementing a random search\n",
    "\n",
    "A first possible algorithm is to sample a change for x and keep the best value.\n",
    "We iterate the following steps : \n",
    "- take a neighbor for x, sampling a random number with standard variation 0.01.\n",
    "- evaluate these two possibilities\n",
    "- move to the best one\n",
    "\n",
    "Implement that with a for loop with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "x = x_0\n",
    "all_results = list()\n",
    "\n",
    "\n",
    "def sample_around(x):\n",
    "    return x + np.random.normal(scale=0.01)\n",
    "\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    sample = sample_around(x)\n",
    "    f_x, f_sample = f(x), f(sample)\n",
    "    # The best value is the sampled one\n",
    "    if f_sample < f_x:\n",
    "        x = sample\n",
    "        all_results.append(f_sample)\n",
    "    # The best one is the former one\n",
    "    else:\n",
    "        x = x\n",
    "        all_results.append(f_x)\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Implementing an exaustive search\n",
    "\n",
    "A first possible algorithm is to try all changes for x and keep the best value.\n",
    "We iterate the following steps : \n",
    "- try a smaller and a larger x value of 0.01.\n",
    "- evaluate these two possibilities\n",
    "- move to the best one\n",
    "\n",
    "Implement that with a for loop with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "x = x_0\n",
    "all_results = list()\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    smaller, larger = x - 0.01, x + 0.01\n",
    "    # ...\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Implementing a gradient descent 'by hand'\n",
    "Now let us implement the gradient descent, by remembering that $\\frac{df}{dx} = 2x$\n",
    "\n",
    "We iterate the following steps : \n",
    "- compute the gradient value at x\n",
    "- Update x : $x \\leftarrow x - 0.01 \\frac{df}{dx}$\n",
    "\n",
    "Implement that with a for loop with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Implementing a gradient descent with automatic differentiation (by hand)\n",
    "\n",
    "We want to use the same algorithm but without knowing the formula of differentiation. \n",
    "We instead want to rely on Pytorch\n",
    "\n",
    "Implement the same method as before, with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    f_x = x ** 2\n",
    "    f_x.backward()\n",
    "    x.data = x - 0.01 * x.grad.item()\n",
    "    x.grad = None\n",
    "    all_results.append(f_x.data)\n",
    "\n",
    "print(x.item())\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Implementing a gradient descent with automatic differentiation (the proper way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "opt = torch.optim.SGD([x], lr=0.01, momentum=0)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    f_x = f(x)\n",
    "    f_x.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    all_results.append(f_x.data)\n",
    "\n",
    "print(x.item())\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Bigger input space\n",
    "\n",
    "Let us now look at a more complicated input space, the function takes as input five numbers and returns :\n",
    "$f_2(x_1, x_2, x_3, x_4, x_5) = (x_1 + x_2 + x_3 + x_4 + x_5)^2$\n",
    "\n",
    "Now it is more costly to find the right direction randomly. Try the random algorithm on this new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2(x):\n",
    "    return (x[0] + x[1] + x[2] + x[3] + x[4]) ** 2\n",
    "\n",
    "\n",
    "new_x_0 = (1, 2, 3, 4, 5)\n",
    "f_2(new_x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "x = new_x_0\n",
    "all_results = list()\n",
    "\n",
    "\n",
    "def sample_around(x):\n",
    "    return x + np.random.normal(size=5, scale=0.01)\n",
    "\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Now let us try the gradient approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "x = torch.tensor(new_x_0, requires_grad=True, dtype=float)\n",
    "opt = torch.optim.SGD([x], lr=0.01, momentum=0)\n",
    "\n",
    "print(x.data)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Actual machine learning examples\n",
    "\n",
    "Now instead of minimizing random functions, let us minimize the error of a linear model !\n",
    "\n",
    "We will simply use the example from the first class. Let us generate the data once again and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(20)\n",
    "\n",
    "\n",
    "def base_function(x):\n",
    "    y = 1.3 * x ** 3 - 3 * x ** 2 + 3.6 * x + 6.9\n",
    "    return y\n",
    "\n",
    "\n",
    "low, high = -1, 3\n",
    "n_points = 100\n",
    "\n",
    "# Get the values\n",
    "xs = np.random.uniform(low, high, n_points)\n",
    "ys_noise = np.random.normal(size=len(xs))\n",
    "sample_ys = base_function(xs)\n",
    "noisy_sample_ys = sample_ys + ys_noise\n",
    "\n",
    "# Plot the hidden function\n",
    "lsp = np.linspace(low, high)[:, None]\n",
    "true_ys = base_function(lsp)\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "\n",
    "# Plot the samples\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Gradient descent using torch.\n",
    "First create a torch version of these objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_noisy_sample_ys = torch.from_numpy(noisy_sample_ys)\n",
    "torch_xs = torch.from_numpy(xs)\n",
    "torch_lsp = torch.from_numpy(lsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Let us try to fit a linear model by hand, instead of simply relying on scikit-learn !\n",
    "\n",
    "The model of a linear regression is : $f_\\theta (x) = (\\theta_1 x + \\theta_0)$\n",
    "\n",
    "Careful ! We do not want to minimize the function of x itself. \n",
    "\n",
    "We want to minimise the errors we make, also called the loss function. We will do this by adjusting the parameters $\\theta$ of the function, starting from an arbitrary value of (1,1). This loss function is the sum of the square errors at each point : \n",
    "\n",
    "$$ \\min_{\\theta}\\mathcal{L} (\\theta) = 1/N\\sum_i (y_i - f_{\\theta} (x_i))^ 2 \\\\\n",
    "= 1/N\\sum_i (y_i - (\\theta_1 x_i + \\theta_0))^ 2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_theta(x, theta):\n",
    "    return theta[1] * x + theta[0]\n",
    "\n",
    "\n",
    "def loss_function(theta):\n",
    "    return torch.mean((torch_noisy_sample_ys - f_theta(torch_xs, theta)) ** 2)\n",
    "\n",
    "\n",
    "initial_theta = torch.tensor((1., 1.), requires_grad=True)\n",
    "initial_loss = loss_function(initial_theta)\n",
    "print(initial_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "theta = copy.deepcopy(initial_theta)\n",
    "opt = torch.optim.SGD([theta], lr=0.01, momentum=0.0)\n",
    "\n",
    "# Implement optimization here, as we did above\n",
    "# ...\n",
    "\n",
    "print(theta.data)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "We have values for the parameters now. \n",
    "Let us look at what they look like.\n",
    "\n",
    "Use the theta_function on the linspace to plot your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ys = f_theta(torch_lsp, theta).detach().numpy()\n",
    "\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "plt.plot(lsp, predicted_ys)\n",
    "\n",
    "# Plot the samples\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
